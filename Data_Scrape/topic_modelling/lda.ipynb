{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../Dataset/BOW.txt\", \"r\")\n",
    "\n",
    "for line in f:\n",
    "    list_of_words = line.split(',')\n",
    "f.close()\n",
    "\n",
    "index_to_words = {}\n",
    "for i in range(len(list_of_words)):\n",
    "    index_to_words[i+1] = list_of_words[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../Dataset/dataset.pkl\", \"rb\")\n",
    "msdid_to_lyrics =  pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "msd_ids = list(msdid_to_lyrics.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = []\n",
    "\n",
    "for msd_id in msd_ids:\n",
    "    lyrics = msdid_to_lyrics[msd_id][-1]\n",
    "\n",
    "    word_lyrics = []\n",
    "    for c in lyrics:\n",
    "        collon_index = c.find(':')\n",
    "        ind = int(c[:collon_index])\n",
    "        freq = int(c[collon_index+1:])\n",
    "        for i in range(freq):\n",
    "            word_lyrics.append(index_to_words[ind])\n",
    "    processed_docs.append(word_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237662\n"
     ]
    }
   ],
   "source": [
    "print(len(processed_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 a\n",
      "1 am\n",
      "2 and\n",
      "3 are\n",
      "4 arrang\n",
      "5 be\n",
      "6 been\n",
      "7 captur\n",
      "8 damn\n",
      "9 de\n",
      "10 devast\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=20, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.save(\"lda_Model_topics_20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.050*\"ha\" + 0.044*\"jag\" + 0.041*\"du\" + 0.036*\"det\" + 0.034*\"och\" + 0.027*\"en\" + 0.026*\"som\" + 0.023*\"bye\" + 0.021*\"Ã¤r\" + 0.021*\"pÃ¥\"'), (1, '0.038*\"burn\" + 0.036*\"fire\" + 0.030*\"your\" + 0.023*\"run\" + 0.017*\"black\" + 0.016*\"like\" + 0.014*\"red\" + 0.012*\"white\" + 0.012*\"with\" + 0.012*\"kill\"'), (2, '0.049*\"rock\" + 0.046*\"lord\" + 0.044*\"sing\" + 0.040*\"god\" + 0.039*\"roll\" + 0.023*\"music\" + 0.023*\"readi\" + 0.022*\"heaven\" + 0.021*\"song\" + 0.020*\"soul\"'), (3, '0.055*\"e\" + 0.032*\"che\" + 0.032*\"na\" + 0.026*\"non\" + 0.025*\"di\" + 0.025*\"da\" + 0.023*\"o\" + 0.019*\"la\" + 0.018*\"un\" + 0.018*\"il\"'), (4, '0.077*\"do\" + 0.033*\"what\" + 0.032*\"know\" + 0.024*\"be\" + 0.023*\"want\" + 0.019*\"if\" + 0.016*\"say\" + 0.015*\"your\" + 0.015*\"can\" + 0.014*\"just\"'), (5, '0.019*\"like\" + 0.018*\"get\" + 0.018*\"up\" + 0.017*\"we\" + 0.015*\"am\" + 0.015*\"with\" + 0.014*\"got\" + 0.013*\"do\" + 0.011*\"your\" + 0.011*\"ya\"'), (6, '0.050*\"down\" + 0.038*\"go\" + 0.035*\"your\" + 0.026*\"let\" + 0.021*\"take\" + 0.020*\"up\" + 0.018*\"get\" + 0.018*\"feel\" + 0.016*\"back\" + 0.015*\"can\"'), (7, '0.065*\"he\" + 0.042*\"was\" + 0.027*\"his\" + 0.020*\"man\" + 0.019*\"him\" + 0.017*\"they\" + 0.012*\"said\" + 0.012*\"but\" + 0.012*\"would\" + 0.011*\"had\"'), (8, '0.296*\"oh\" + 0.174*\"yeah\" + 0.023*\"babi\" + 0.021*\"yes\" + 0.017*\"whoa\" + 0.014*\"uh\" + 0.014*\"love\" + 0.012*\"christma\" + 0.010*\"happi\" + 0.009*\"ohh\"'), (9, '0.118*\"she\" + 0.070*\"her\" + 0.064*\"babi\" + 0.045*\"girl\" + 0.042*\"love\" + 0.019*\"littl\" + 0.016*\"got\" + 0.015*\"danc\" + 0.014*\"like\" + 0.013*\"with\"'), (10, '0.511*\"no\" + 0.100*\"more\" + 0.028*\"wo\" + 0.025*\"ja\" + 0.023*\"ai\" + 0.012*\"ei\" + 0.010*\"ni\" + 0.008*\"there\" + 0.007*\"kun\" + 0.007*\"se\"'), (11, '0.082*\"ah\" + 0.036*\"u\" + 0.034*\"ik\" + 0.030*\"de\" + 0.028*\"2\" + 0.022*\"en\" + 0.022*\"dem\" + 0.022*\"je\" + 0.020*\"het\" + 0.020*\"1\"'), (12, '0.288*\"come\" + 0.143*\"hey\" + 0.136*\"la\" + 0.075*\"ooh\" + 0.020*\"here\" + 0.017*\"doo\" + 0.016*\"yea\" + 0.014*\"ho\" + 0.011*\"back\" + 0.007*\"dee\"'), (13, '0.059*\"am\" + 0.032*\"have\" + 0.025*\"are\" + 0.017*\"get\" + 0.017*\"out\" + 0.017*\"but\" + 0.015*\"time\" + 0.013*\"just\" + 0.013*\"got\" + 0.012*\"so\"'), (14, '0.061*\"que\" + 0.042*\"de\" + 0.042*\"y\" + 0.033*\"la\" + 0.032*\"no\" + 0.031*\"el\" + 0.023*\"en\" + 0.021*\"te\" + 0.019*\"mi\" + 0.018*\"tu\"'), (15, '0.054*\"ich\" + 0.045*\"und\" + 0.039*\"die\" + 0.027*\"du\" + 0.024*\"der\" + 0.023*\"nicht\" + 0.020*\"das\" + 0.019*\"ist\" + 0.018*\"es\" + 0.018*\"ein\"'), (16, '0.041*\"love\" + 0.041*\"we\" + 0.038*\"will\" + 0.024*\"your\" + 0.022*\"be\" + 0.020*\"are\" + 0.017*\"for\" + 0.015*\"so\" + 0.014*\"have\" + 0.014*\"this\"'), (17, '0.025*\"your\" + 0.023*\"we\" + 0.019*\"are\" + 0.017*\"for\" + 0.014*\"this\" + 0.013*\"they\" + 0.013*\"will\" + 0.010*\"with\" + 0.010*\"our\" + 0.010*\"from\"'), (18, '0.045*\"de\" + 0.037*\"la\" + 0.032*\"le\" + 0.029*\"et\" + 0.028*\"je\" + 0.028*\"les\" + 0.019*\"pas\" + 0.019*\"que\" + 0.017*\"des\" + 0.017*\"un\"'), (19, '0.012*\"light\" + 0.012*\"will\" + 0.012*\"night\" + 0.012*\"as\" + 0.012*\"day\" + 0.011*\"from\" + 0.010*\"dream\" + 0.010*\"with\" + 0.009*\"there\" + 0.009*\"sky\"')]\n"
     ]
    }
   ],
   "source": [
    "print(lda_model.print_topics())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.6260613917208783\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda_model.get_document_topics(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "[2071, 3143, 2232, 8222, 45031, 18172, 14320, 18247, 1468, 8666, 2066, 1564, 1002, 44893, 15242, 5176, 54510, 40761, 6748, 31641]\n"
     ]
    }
   ],
   "source": [
    "topic_count = []\n",
    "for i in range(20):\n",
    "    topic_count.append(0)\n",
    "list1 = []\n",
    "for i in range(len(topics)):\n",
    "    a = []\n",
    "    for c in topics[i]:\n",
    "        if c[1] >0.24999:\n",
    "            a.append(c[0])\n",
    "    list1.append(a)\n",
    "    for j in range(20):\n",
    "        if j in a:\n",
    "            topic_count[j] += 1\n",
    "    if(i%10000 == 0):\n",
    "        print(i)\n",
    "print(topic_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
