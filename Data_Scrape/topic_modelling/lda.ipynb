{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gensim\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../Dataset/BOW.txt\", \"r\")\n",
    "\n",
    "for line in f:\n",
    "    list_of_words = line.split(',')\n",
    "f.close()\n",
    "\n",
    "index_to_words = {}\n",
    "for i in range(len(list_of_words)):\n",
    "    index_to_words[i+1] = list_of_words[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../Dataset/dataset.pkl\", \"rb\")\n",
    "msdid_to_lyrics =  pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "msd_ids = list(msdid_to_lyrics.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "languages_list = ['english', 'spanish', 'german', 'french', 'italian']\n",
    "for language in languages_list:\n",
    "    stop_words.extend(stopwords.words(language))\n",
    "\n",
    "from nltk.stem import PorterStemmer \n",
    "ps = PorterStemmer() \n",
    "\n",
    "stop_words = [ps.stem(w) for w in stop_words]\n",
    "# print(stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = []\n",
    "\n",
    "for msd_id in msd_ids:\n",
    "    lyrics = msdid_to_lyrics[msd_id][-1]\n",
    "\n",
    "    word_lyrics = []\n",
    "    for c in lyrics:\n",
    "        collon_index = c.find(':')\n",
    "        ind = int(c[:collon_index])\n",
    "        freq = int(c[collon_index+1:])\n",
    "        for i in range(freq):\n",
    "            w = index_to_words[ind]\n",
    "            if len(w)<=3 or w in stop_words:\n",
    "                continue\n",
    "            word_lyrics.append(w)\n",
    "    processed_docs.append(word_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237662\n"
     ]
    }
   ],
   "source": [
    "print(len(processed_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 arrang\n",
      "1 captur\n",
      "2 damn\n",
      "3 devast\n",
      "4 element\n",
      "5 fashion\n",
      "6 flesh\n",
      "7 grace\n",
      "8 grant\n",
      "9 highest\n",
      "10 ignor\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaModel(bow_corpus, num_topics=20, id2word=dictionary, passes=2, alpha = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.save(\"lda_Model_topics_20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.5724731936938867\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.054*\"solo\" + 0.036*\"piÃ¹\" + 0.034*\"quando\" + 0.022*\"sempr\" + 0.022*\"senza\" + 0.020*\"tempo\" + 0.018*\"ancora\" + 0.017*\"dentro\" + 0.017*\"cosa\" + 0.017*\"vita\"'), (1, '0.089*\"world\" + 0.058*\"sing\" + 0.054*\"song\" + 0.051*\"live\" + 0.041*\"peopl\" + 0.028*\"togeth\" + 0.026*\"bring\" + 0.025*\"hear\" + 0.024*\"shine\" + 0.024*\"stand\"'), (2, '0.074*\"time\" + 0.054*\"feel\" + 0.050*\"wanna\" + 0.049*\"girl\" + 0.048*\"know\" + 0.042*\"make\" + 0.041*\"take\" + 0.037*\"right\" + 0.032*\"good\" + 0.031*\"tell\"'), (3, '0.055*\"littl\" + 0.029*\"ride\" + 0.022*\"star\" + 0.022*\"water\" + 0.018*\"like\" + 0.015*\"sweet\" + 0.014*\"wild\" + 0.013*\"river\" + 0.012*\"white\" + 0.012*\"child\"'), (4, '0.211*\"heart\" + 0.082*\"break\" + 0.041*\"beat\" + 0.033*\"anoth\" + 0.033*\"apart\" + 0.031*\"tear\" + 0.030*\"broken\" + 0.016*\"part\" + 0.012*\"woah\" + 0.011*\"toujour\"'), (5, '0.039*\"like\" + 0.017*\"this\" + 0.017*\"nigga\" + 0.014*\"shit\" + 0.013*\"fuck\" + 0.013*\"rock\" + 0.011*\"back\" + 0.011*\"caus\" + 0.009*\"money\" + 0.008*\"roll\"'), (6, '0.042*\"know\" + 0.038*\"would\" + 0.034*\"never\" + 0.026*\"could\" + 0.022*\"think\" + 0.020*\"thing\" + 0.018*\"like\" + 0.016*\"this\" + 0.015*\"said\" + 0.013*\"look\"'), (7, '0.229*\"want\" + 0.058*\"around\" + 0.046*\"turn\" + 0.042*\"move\" + 0.041*\"play\" + 0.033*\"like\" + 0.032*\"round\" + 0.031*\"make\" + 0.025*\"free\" + 0.018*\"easi\"'), (8, '0.157*\"yeah\" + 0.149*\"babi\" + 0.032*\"gotta\" + 0.025*\"well\" + 0.019*\"know\" + 0.017*\"shake\" + 0.017*\"gonna\" + 0.017*\"like\" + 0.016*\"crazi\" + 0.015*\"woman\"'), (9, '0.053*\"cest\" + 0.039*\"tout\" + 0.034*\"plus\" + 0.032*\"comm\" + 0.025*\"nous\" + 0.021*\"quand\" + 0.017*\"refrain\" + 0.017*\"fait\" + 0.016*\"bien\" + 0.016*\"rien\"'), (10, '0.037*\"fÃ¼r\" + 0.033*\"mehr\" + 0.027*\"immer\" + 0.022*\"bang\" + 0.022*\"schon\" + 0.021*\"lang\" + 0.021*\"lieb\" + 0.020*\"leben\" + 0.020*\"sunday\" + 0.019*\"zeit\"'), (11, '0.016*\"hand\" + 0.013*\"land\" + 0.012*\"rise\" + 0.011*\"earth\" + 0.011*\"must\" + 0.010*\"mother\" + 0.009*\"father\" + 0.009*\"brother\" + 0.008*\"power\" + 0.007*\"shall\"'), (12, '0.202*\"gonna\" + 0.117*\"night\" + 0.107*\"tonight\" + 0.096*\"danc\" + 0.058*\"alright\" + 0.043*\"light\" + 0.023*\"parti\" + 0.014*\"jump\" + 0.013*\"tight\" + 0.011*\"sleep\"'), (13, '0.135*\"beauti\" + 0.083*\"till\" + 0.081*\"talk\" + 0.052*\"christma\" + 0.026*\"nacht\" + 0.025*\"glad\" + 0.020*\"hero\" + 0.020*\"liar\" + 0.017*\"special\" + 0.016*\"fÃ¶r\"'), (14, '0.049*\"this\" + 0.026*\"life\" + 0.017*\"feel\" + 0.016*\"time\" + 0.014*\"fall\" + 0.013*\"insid\" + 0.013*\"take\" + 0.011*\"live\" + 0.011*\"mind\" + 0.010*\"away\"'), (15, '0.035*\"amor\" + 0.022*\"vida\" + 0.019*\"quiero\" + 0.015*\"nÃ£o\" + 0.015*\"mÃ¡s\" + 0.010*\"nunca\" + 0.010*\"siempr\" + 0.009*\"tiempo\" + 0.008*\"estÃ¡\" + 0.008*\"corazÃ³n\"'), (16, '0.050*\"fire\" + 0.045*\"burn\" + 0.042*\"lord\" + 0.039*\"soul\" + 0.035*\"heaven\" + 0.025*\"save\" + 0.023*\"angel\" + 0.021*\"hell\" + 0.017*\"pray\" + 0.017*\"flame\"'), (17, '0.035*\"dead\" + 0.032*\"kill\" + 0.024*\"head\" + 0.022*\"black\" + 0.022*\"blood\" + 0.016*\"fight\" + 0.014*\"hate\" + 0.013*\"lyric\" + 0.013*\"readi\" + 0.013*\"death\"'), (18, '0.034*\"away\" + 0.028*\"home\" + 0.020*\"long\" + 0.020*\"night\" + 0.019*\"walk\" + 0.019*\"gone\" + 0.018*\"back\" + 0.015*\"wait\" + 0.015*\"rain\" + 0.015*\"blue\"'), (19, '0.274*\"love\" + 0.047*\"need\" + 0.038*\"give\" + 0.029*\"know\" + 0.024*\"heart\" + 0.018*\"hold\" + 0.016*\"believ\" + 0.015*\"feel\" + 0.015*\"true\" + 0.013*\"kiss\"')]\n"
     ]
    }
   ],
   "source": [
    "print(lda_model.print_topics())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = lda_model.get_document_topics(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "[4867, 3639, 15470, 5433, 1547, 20110, 67794, 2843, 7068, 7120, 5282, 10404, 1953, 1819, 53261, 18932, 4749, 5945, 27970, 15589]\n"
     ]
    }
   ],
   "source": [
    "topic_count = []\n",
    "for i in range(20):\n",
    "    topic_count.append(0)\n",
    "list1 = []\n",
    "for i in range(len(res)):\n",
    "    a = []\n",
    "    for c in res[i]:\n",
    "        if c[1] >0.24999:\n",
    "            a.append(c[0])\n",
    "    list1.append(a)\n",
    "    for j in range(20):\n",
    "        if j in a:\n",
    "            topic_count[j] += 1\n",
    "            \n",
    "    if(i%10000 == 0):\n",
    "        print(i)\n",
    "print(topic_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_2 = gensim.models.LdaMulticore(bow_corpus, num_topics=40, id2word=dictionary, passes=2, workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_2.save(\"lda_Model_topics_40\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.46624006363405435\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model_2, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(38, '0.094*\"call\" + 0.061*\"better\" + 0.053*\"chorus\" + 0.046*\"name\" + 0.029*\"readi\" + 0.029*\"hear\" + 0.021*\"hous\" + 0.021*\"brother\" + 0.018*\"line\" + 0.016*\"number\"'), (13, '0.018*\"soul\" + 0.018*\"blood\" + 0.017*\"burn\" + 0.016*\"kill\" + 0.016*\"death\" + 0.011*\"fire\" + 0.010*\"this\" + 0.009*\"dead\" + 0.008*\"rise\" + 0.008*\"hell\"'), (0, '0.063*\"cool\" + 0.039*\"shut\" + 0.032*\"niet\" + 0.027*\"anim\" + 0.026*\"vain\" + 0.025*\"fever\" + 0.025*\"rockin\" + 0.025*\"maar\" + 0.024*\"minut\" + 0.023*\"monkey\"'), (7, '0.180*\"night\" + 0.126*\"danc\" + 0.111*\"fall\" + 0.028*\"christma\" + 0.028*\"ladi\" + 0.022*\"send\" + 0.020*\"white\" + 0.012*\"floor\" + 0.011*\"dream\" + 0.009*\"miracl\"'), (39, '0.088*\"look\" + 0.078*\"around\" + 0.060*\"everi\" + 0.045*\"break\" + 0.026*\"breath\" + 0.025*\"make\" + 0.022*\"turn\" + 0.017*\"enough\" + 0.016*\"ground\" + 0.014*\"noth\"'), (30, '0.588*\"yeah\" + 0.151*\"wait\" + 0.033*\"beat\" + 0.013*\"rhythm\" + 0.008*\"loos\" + 0.007*\"long\" + 0.006*\"time\" + 0.006*\"kick\" + 0.006*\"ohoh\" + 0.005*\"dancin\"'), (15, '0.207*\"believ\" + 0.093*\"help\" + 0.072*\"money\" + 0.042*\"honey\" + 0.021*\"problem\" + 0.020*\"poor\" + 0.018*\"mess\" + 0.018*\"count\" + 0.017*\"mother\" + 0.017*\"rather\"'), (35, '0.086*\"lord\" + 0.078*\"high\" + 0.039*\"watch\" + 0.038*\"move\" + 0.037*\"jesus\" + 0.026*\"pray\" + 0.023*\"children\" + 0.021*\"holi\" + 0.020*\"lift\" + 0.020*\"mountain\"'), (2, '0.217*\"need\" + 0.095*\"alway\" + 0.081*\"find\" + 0.058*\"mind\" + 0.042*\"lose\" + 0.041*\"someon\" + 0.037*\"round\" + 0.028*\"sometim\" + 0.021*\"know\" + 0.014*\"control\"'), (10, '0.232*\"live\" + 0.106*\"bring\" + 0.086*\"lone\" + 0.060*\"road\" + 0.041*\"lyric\" + 0.040*\"born\" + 0.036*\"summer\" + 0.026*\"gimm\" + 0.023*\"babe\" + 0.016*\"half\"'), (17, '0.223*\"gone\" + 0.123*\"beauti\" + 0.066*\"young\" + 0.050*\"done\" + 0.044*\"face\" + 0.032*\"radio\" + 0.024*\"iâ€™m\" + 0.018*\"donâ€™t\" + 0.017*\"itâ€™\" + 0.017*\"circl\"'), (14, '0.242*\"stand\" + 0.124*\"mani\" + 0.056*\"magic\" + 0.048*\"devil\" + 0.030*\"paradis\" + 0.024*\"generat\" + 0.020*\"action\" + 0.020*\"band\" + 0.019*\"revolut\" + 0.018*\"march\"'), (27, '0.022*\"like\" + 0.022*\"nigga\" + 0.021*\"fuck\" + 0.020*\"this\" + 0.019*\"shit\" + 0.012*\"caus\" + 0.011*\"bitch\" + 0.011*\"know\" + 0.010*\"back\" + 0.008*\"yall\"'), (12, '0.062*\"ride\" + 0.062*\"blue\" + 0.041*\"anoth\" + 0.025*\"side\" + 0.024*\"somebodi\" + 0.021*\"open\" + 0.017*\"fight\" + 0.017*\"slow\" + 0.016*\"drive\" + 0.015*\"follow\"'), (31, '0.146*\"girl\" + 0.034*\"fÃ¼r\" + 0.029*\"mehr\" + 0.024*\"immer\" + 0.019*\"schon\" + 0.019*\"leben\" + 0.019*\"lieb\" + 0.017*\"komm\" + 0.017*\"zeit\" + 0.016*\"nacht\"'), (22, '0.198*\"tonight\" + 0.089*\"real\" + 0.069*\"matter\" + 0.067*\"angel\" + 0.034*\"reason\" + 0.032*\"child\" + 0.026*\"sunshin\" + 0.022*\"fine\" + 0.021*\"lovin\" + 0.020*\"welcom\"'), (4, '0.338*\"feel\" + 0.143*\"littl\" + 0.053*\"make\" + 0.041*\"hard\" + 0.032*\"anyth\" + 0.024*\"happen\" + 0.018*\"like\" + 0.018*\"closer\" + 0.012*\"insid\" + 0.011*\"grow\"'), (26, '0.478*\"love\" + 0.021*\"heart\" + 0.021*\"good\" + 0.021*\"give\" + 0.020*\"know\" + 0.019*\"true\" + 0.015*\"woman\" + 0.013*\"make\" + 0.011*\"sweet\" + 0.010*\"much\"'), (25, '0.069*\"light\" + 0.033*\"turn\" + 0.030*\"dark\" + 0.025*\"shine\" + 0.023*\"night\" + 0.022*\"dead\" + 0.018*\"aliv\" + 0.018*\"insid\" + 0.016*\"shadow\" + 0.014*\"burn\"'), (19, '0.037*\"long\" + 0.029*\"time\" + 0.028*\"last\" + 0.023*\"rememb\" + 0.018*\"still\" + 0.018*\"this\" + 0.017*\"goodby\" + 0.016*\"could\" + 0.015*\"year\" + 0.014*\"never\"')]\n"
     ]
    }
   ],
   "source": [
    "print(lda_model_2.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_3 = gensim.models.LdaMulticore(bow_corpus, num_topics=20, id2word=dictionary, passes=2, workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_3.save(\"lda_Model_topics_20_alpha_not_def\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.5868904872668171\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model_3, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.075*\"danc\" + 0.040*\"fire\" + 0.028*\"citi\" + 0.025*\"black\" + 0.024*\"burn\" + 0.024*\"move\" + 0.022*\"parti\" + 0.019*\"white\" + 0.017*\"water\" + 0.015*\"town\"'), (1, '0.034*\"like\" + 0.018*\"nigga\" + 0.017*\"fuck\" + 0.017*\"this\" + 0.015*\"shit\" + 0.011*\"caus\" + 0.009*\"money\" + 0.009*\"back\" + 0.009*\"bitch\" + 0.009*\"know\"'), (2, '0.153*\"gonna\" + 0.119*\"wanna\" + 0.063*\"give\" + 0.063*\"make\" + 0.053*\"gotta\" + 0.025*\"ride\" + 0.024*\"turn\" + 0.023*\"round\" + 0.018*\"take\" + 0.017*\"around\"'), (3, '0.026*\"would\" + 0.016*\"could\" + 0.015*\"home\" + 0.013*\"said\" + 0.012*\"like\" + 0.012*\"time\" + 0.011*\"this\" + 0.011*\"never\" + 0.009*\"night\" + 0.009*\"look\"'), (4, '0.030*\"amor\" + 0.021*\"quiero\" + 0.019*\"vida\" + 0.015*\"mÃ¡s\" + 0.011*\"solo\" + 0.011*\"siempr\" + 0.010*\"nunca\" + 0.009*\"tiempo\" + 0.008*\"corazÃ³n\" + 0.008*\"ahora\"'), (5, '0.044*\"cest\" + 0.033*\"tout\" + 0.028*\"plus\" + 0.026*\"comm\" + 0.020*\"nous\" + 0.017*\"quand\" + 0.014*\"fait\" + 0.014*\"bien\" + 0.013*\"refrain\" + 0.013*\"rien\"'), (6, '0.019*\"light\" + 0.013*\"dark\" + 0.012*\"dream\" + 0.011*\"burn\" + 0.011*\"night\" + 0.010*\"soul\" + 0.009*\"fall\" + 0.009*\"death\" + 0.008*\"like\" + 0.008*\"this\"'), (7, '0.117*\"rock\" + 0.091*\"shake\" + 0.078*\"roll\" + 0.051*\"whoa\" + 0.048*\"nÃ£o\" + 0.040*\"somebodi\" + 0.022*\"vocÃª\" + 0.019*\"radio\" + 0.018*\"bodi\" + 0.014*\"rockin\"'), (8, '0.033*\"fÃ¼r\" + 0.028*\"mehr\" + 0.024*\"immer\" + 0.019*\"rain\" + 0.019*\"schon\" + 0.019*\"leben\" + 0.019*\"lieb\" + 0.018*\"woah\" + 0.017*\"nacht\" + 0.017*\"oooh\"'), (9, '0.128*\"babi\" + 0.037*\"good\" + 0.028*\"night\" + 0.021*\"time\" + 0.019*\"right\" + 0.018*\"long\" + 0.015*\"well\" + 0.015*\"yeah\" + 0.014*\"know\" + 0.014*\"woman\"'), (10, '0.029*\"piÃ¹\" + 0.028*\"solo\" + 0.021*\"quando\" + 0.017*\"senza\" + 0.016*\"niet\" + 0.014*\"sempr\" + 0.014*\"vita\" + 0.014*\"ancora\" + 0.013*\"maar\" + 0.013*\"dentro\"'), (11, '0.042*\"sing\" + 0.040*\"tonight\" + 0.037*\"song\" + 0.029*\"blue\" + 0.025*\"keep\" + 0.023*\"sweet\" + 0.019*\"everybodi\" + 0.019*\"hear\" + 0.015*\"alright\" + 0.014*\"call\"'), (12, '0.297*\"believ\" + 0.222*\"anoth\" + 0.105*\"work\" + 0.037*\"gimm\" + 0.019*\"miracl\" + 0.017*\"surrend\" + 0.015*\"anim\" + 0.014*\"vamo\" + 0.013*\"sometim\" + 0.010*\"bigger\"'), (13, '0.073*\"this\" + 0.021*\"world\" + 0.016*\"noth\" + 0.013*\"never\" + 0.012*\"word\" + 0.011*\"time\" + 0.010*\"feel\" + 0.009*\"thing\" + 0.008*\"life\" + 0.008*\"hate\"'), (14, '0.096*\"want\" + 0.079*\"girl\" + 0.064*\"back\" + 0.050*\"littl\" + 0.023*\"play\" + 0.015*\"look\" + 0.012*\"bring\" + 0.009*\"tell\" + 0.009*\"happi\" + 0.008*\"till\"'), (15, '0.103*\"know\" + 0.063*\"yeah\" + 0.047*\"like\" + 0.030*\"feel\" + 0.029*\"think\" + 0.021*\"tell\" + 0.020*\"thing\" + 0.020*\"caus\" + 0.017*\"want\" + 0.017*\"right\"'), (16, '0.030*\"lord\" + 0.024*\"heaven\" + 0.018*\"angel\" + 0.016*\"king\" + 0.013*\"jesus\" + 0.013*\"high\" + 0.013*\"name\" + 0.013*\"born\" + 0.012*\"save\" + 0.010*\"pray\"'), (17, '0.024*\"stop\" + 0.019*\"kill\" + 0.013*\"beat\" + 0.012*\"lyric\" + 0.012*\"dead\" + 0.012*\"blood\" + 0.009*\"control\" + 0.009*\"power\" + 0.007*\"fight\" + 0.007*\"human\"'), (18, '0.048*\"time\" + 0.048*\"take\" + 0.043*\"away\" + 0.038*\"life\" + 0.024*\"live\" + 0.017*\"mind\" + 0.016*\"find\" + 0.015*\"back\" + 0.013*\"leav\" + 0.012*\"chang\"'), (19, '0.154*\"love\" + 0.034*\"never\" + 0.031*\"heart\" + 0.026*\"need\" + 0.026*\"know\" + 0.025*\"would\" + 0.018*\"feel\" + 0.017*\"could\" + 0.014*\"ever\" + 0.013*\"want\"')]\n"
     ]
    }
   ],
   "source": [
    "print(lda_model_3.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = lda_model_3.get_document_topics(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "[3927, 17922, 4574, 46871, 15305, 6638, 31599, 3862, 5480, 13394, 5103, 8769, 506, 34511, 9236, 26340, 10071, 10432, 29265, 33937]\n"
     ]
    }
   ],
   "source": [
    "topic_count = []\n",
    "for i in range(20):\n",
    "    topic_count.append(0)\n",
    "list1 = []\n",
    "for i in range(len(res2)):\n",
    "    a = []\n",
    "    for c in res2[i]:\n",
    "        if c[1] >0.24999:\n",
    "            a.append(c[0])\n",
    "    list1.append(a)\n",
    "    for j in range(20):\n",
    "        if j in a:\n",
    "            topic_count[j] += 1\n",
    "            \n",
    "    if(i%10000 == 0):\n",
    "        print(i)\n",
    "print(topic_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
